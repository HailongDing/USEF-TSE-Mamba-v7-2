# USEF-TSE-Mamba V7 Optimized Configuration
# For use with 30K/2K/1K dataset configuration

# Model Configuration - Enhanced Model for V7
model:
  dim: 128                      # Base dimension
  kernel_sizes: [16, 16, 16]    # Conv kernel sizes for each stage
  strides: [2, 2, 2]            # Downsampling strides
  num_blocks: 6                 # Number of Mamba blocks per stage
  activation: silu              # SiLU activation for better gradient flow
  num_heads: 8                  # Number of attention heads for cross-attention
  dropout: 0.2                  # Dropout for all layers (including attention)

  # V7 Enhanced Model Features
  use_bottleneck_attention_only: true  # More efficient attention (only at bottleneck)
  use_layer_norm: true          # Apply layer normalization
  use_residual: true            # Use residual connections
  attention_temperature: 1.0    # Temperature for attention softmax

# Data Configuration - OPTIMIZED FOR 30K/2K/1K
data:
  # Dataset paths - Point to optimized dataset
  data_root: /data/tse_audio_dataset_v7_optimized    # Dataset location
  use_metadata: true                  # Use CSV metadata files
  metadata_path: /data/tse_audio_dataset_v7_optimized/metadata  # Metadata in dataset folder

  # Audio parameters - Extended for better temporal context
  sample_rate: 8000            # Sample rate in Hz
  segment_length: 6.0          # Extended from 4.0s for better context learning

  # Reference audio settings - Extended for richer speaker features
  ref_length_min: 4.0          # Extended from 2.5s for better speaker characterization
  ref_length_max: 4.0          # Fixed reference length

  # Variable reference configuration
  use_variable_ref: false      # Disabled - using fixed references for stability
  ref_length_strategy: adaptive # Strategy: 'adaptive', 'random', or 'progressive'
  memory_safe_mode: true       # Limit max reference to stay within 24GB GPU

  # Data loading - OPTIMIZED
  batch_size: 2                # Keep at 2 for memory safety
  num_workers: 16              # Optimized parallel I/O

  # Default SNR values for fallback
  default_speaker_snr: 5.0     # Default speaker SNR when not in metadata
  default_noise_snr: 10.0      # Default noise SNR when not in metadata

  # Audio normalization
  normalize: false             # Audio is pre-normalized during dataset generation
  normalize_scale: 0.95        # Scale factor (not used when normalize=false)

  # DataLoader settings - OPTIMIZED
  drop_last_train: true        # Drop last incomplete batch in training
  persistent_workers: true     # Keep data loading workers alive between epochs
  pin_memory: true             # Pin memory for faster GPU transfer
  prefetch_factor: 4           # Increased for better buffering

# Training Configuration - OPTIMIZED VALIDATION
training:
  num_epochs: 150             # Total number of training epochs
  grad_clip: 5.0              # Gradient clipping value

  # Mixed precision training (enabled for memory efficiency)
  mixed_precision: true       # Enable automatic mixed precision
  gradient_accumulation: 4    # Increased to 4 for effective batch_size=8

  # OPTIMIZED VALIDATION STRATEGY
  validation_frequency: 5      # Quick validation every 5 epochs
  full_val_frequency: 10      # Full validation every 10 epochs
  quick_val_samples: 500      # Number of samples for quick validation

  # Early stopping - More aggressive to prevent overfitting
  early_stopping:
    enabled: true
    patience: 20              # Adjusted for less frequent validation
    min_delta: 0.001          # Increased threshold for improvement
    monitor: val_si_sdri      # Monitor validation SI-SDRi
    mode: max                 # Maximize SI-SDRi

  # Advanced training strategies
  warmup_epochs: 5            # Warmup epochs for learning rate
  warmup_start_lr: 1.0e-5     # Starting LR for warmup

  # Data Augmentation
  use_augmentation: true      # Enable comprehensive augmentation
  augmentation:
    mixup_alpha: 0.2          # MixUp interpolation parameter
    speed_perturb: 0.1        # Â±10% speed perturbation
    spec_augment: true        # SpecAugment masking
    freq_mask_param: 15       # Frequency mask parameter
    time_mask_param: 20       # Time mask parameter
    noise_augment: true       # Dynamic noise addition
    noise_snr_range: [-5, 15] # Extended SNR range for augmentation

  # Model EMA for better generalization
  use_ema: true               # Enable exponential moving average
  ema_decay: 0.999           # EMA decay rate
  ema_update_every: 10       # Update EMA every N steps

# Optimizer Configuration - Anti-overfitting settings
optimizer:
  type: adamw                 # Optimizer type: adam, adamw
  learning_rate: 0.0001       # Reduced LR to prevent overfitting
  betas: [0.9, 0.999]        # Adam beta parameters
  weight_decay: 0.05         # Increased weight decay for stronger regularization
  label_smoothing: 0.1       # Label smoothing for better generalization

# Learning Rate Scheduler - Cosine Annealing with Warm Restarts
scheduler:
  type: cosine_annealing_warm_restarts  # Better for preventing overfitting
  T_0: 10                     # Initial restart period
  T_mult: 2                   # Period doubling after each restart
  eta_min: 1.0e-6            # Minimum learning rate
  # Fallback for reduce_on_plateau
  factor: 0.5                 # Factor to reduce LR
  patience: 10                # Patience for plateau

# Paths Configuration
paths:
  output_dir: ./outputs/usef_tse_v7_optimized      # Directory for outputs
  checkpoint_dir: ./checkpoints/usef_tse_v7_optimized  # Directory for checkpoints

# Logging Configuration
logging:
  use_tensorboard: true       # Enable TensorBoard logging
  log_interval: 50           # Log every N steps
  save_interval: 5           # Save checkpoint every N epochs

# Hardware Configuration
use_multi_gpu: false         # Use multiple GPUs if available

# Evaluation Configuration
evaluation:
  compute_pesq: false         # Compute PESQ scores
  compute_stoi: false         # Compute STOI scores
  save_audio: true           # Save extracted audio samples
  num_save_samples: 10       # Number of samples to save

  # Enhanced metrics
  compute_sdr: true           # Compute SDR
  compute_sir: true           # Compute SIR
  compute_sar: true           # Compute SAR

# Inference Configuration
inference:
  chunk_size: null            # Set for streaming
  chunk_overlap: 0.5          # Overlap ratio between chunks
  batch_inference: true       # Enable batch processing
  device_map: auto            # Device placement strategy

# DATASET SIZE OPTIMIZATION NOTE
# This config is optimized for:
# - Train: 30,000 samples (50% more data)
# - Val: 2,000 samples (60% less validation overhead)
# - Test: 1,000 samples (sufficient for evaluation)
#
# Benefits:
# - 28% faster training per full run
# - Better convergence with more training data
# - Reduced validation overhead
# - Statistically significant metrics maintained